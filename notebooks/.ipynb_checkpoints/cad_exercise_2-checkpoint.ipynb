{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Implementing the components of logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression classifier can be considered as an extension of linear regression. For a binary classification problem (a classification problem with two classes), logistic regression predicts the probability that a sample **x** belongs to one of the classes:\n",
    "\n",
    "$$\n",
    "p(y=1 | \\mathbf{x})=\\sigma\\left(\\boldsymbol{\\theta}^{\\top} \\mathbf{x}\\right)\n",
    "$$\n",
    "\n",
    "We can view this expression as passing the output from a linear regression model $\\boldsymbol{\\theta}^\\intercal \\mathbf{x}$ through the sigmoid function $\\sigma(\\cdot)$ that \"squashes\" the value between 0 and 1 making it possible to be interpreted as a probability.\n",
    "\n",
    "The loss function for logistic regression is the negative log-likelihood (NLL):\n",
    "\n",
    "$$\n",
    "J(\\theta)=-\\sum_{i=1}^{N} y_{i} \\log p\\left(y=1 | \\mathbf{x}_{i}, \\theta\\right)+\\left(1-y_{i}\\right) \\log \\left\\{1-p\\left(y=1 | \\mathbf{x}_{i}, \\theta\\right)\\right\\}\n",
    "$$\n",
    "\n",
    "Compared to linear regression, there is no closed-form solution for the optimal parameters of the model (we cannot set the derivative of $J(\\boldsymbol\\theta)$ to zero and solve for $\\boldsymbol\\theta$). The NLL loss is optimised with the gradient descent method, similar to intensity-based image registration covered in the Registration topic of this course.\n",
    "\n",
    "The provided `logistic_regression()` Python script in `# SECTION 2` of the `cad_tests.py` module implements all necessary steps for training a logistic regression model on a toy dataset. However, the code will not work as is because two of the functions it depends on (`sigmoid()` and `lr_nll()`) are not implemented yet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A)\n",
    "### EXERCISE:\n",
    "Implement the computation of the sigmoid function in `sigmoid()` in `# SECTION 2` of the `cads.py` module. You will test your implementation in the next exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B)\n",
    "### EXERCISE:\n",
    "Implement the computation of the negative log-likelihood in `lr_nll` in `# SECTION 2` of the `cads.py` module. You will test your implementation in the next exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (C)\n",
    "### QUESTION:\n",
    "Suppose that you have two logistic regression models that predict $p(y=1 | \\mathbf{x})$ and a validation dataset with three samples with labels 1, 0 and 1. The first model predicts the following probabilities for the three validation samples: 0.9, 0.4 and 0.7. The second model predicts 0.7. 0.5 and 0.9. Which of the two models has a better performance on the validation set? How did you come to this conclusion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Implementing logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided `logistic_regression()` Python script implements all necessary steps for training a logistic regression model on a toy dataset. \n",
    "\n",
    "The first part of the script generates and visualises a dataset for a binary classification problem. The code generates both a training and a validation dataset, which can be used to monitor for overfitting  during the training process. The second part implements training of logistic regression with stochastic gradient descent. The training process is visualised in two ways: a scatter plot of the training data along with the linear decision boundary, and a plot of the training and validation loss as a function of the number of iterations (this is similar to the plot of the similarity vs. the number of iteration for intensity-baser image registration).\n",
    "\n",
    "Read through the code and comments and make sure you understand what it does (you can skip the visualisation part as it is not relevant for understanding logistic regression and stochastic gradient desent).\n",
    "\n",
    "If you have implemented `sigmoid()` and `lr_nll()` correctly and run `logistic_regression()`, the results should look like on the figure below (it will most likely not be exactly the same as the toy dataset is randomly generated). \n",
    "\n",
    "<img src=\"../notebooks/assets/logreg_training.png\" align=\"center\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "from IPython.display import display, clear_output, HTML\n",
    "from cad_tests import logistic_regression\n",
    "\n",
    "logistic_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A)\n",
    "### QUESTION:\n",
    "What is the difference between \"regular\" gradient descent and stochastic gradient descent? What is the advantage of one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B)\n",
    "### QUESTION:\n",
    "In the figure above, the training loss curve has a noisy appearance, whereas the validation loss curve is relatively smooth. Why is this the case (HINT: How will the appearance of the training loss curve change if you increase the batch size parameter?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (C)\n",
    "### QUESTION:\n",
    "Based on the training curves in the figure above, would you say that the model has overfitted the training dataset? Motivate your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (D)\n",
    "### QUESTION:\n",
    "Assuming that you have trained a model and are satisfied with the generalisation performance, how can you use the model to predict the class label $y$ for an unknown test sample $\\mathbf{x}$. (HINT: Remember that the model predicts a probability. How can this probability be converted to a binary class label?)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
